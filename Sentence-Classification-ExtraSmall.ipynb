{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing code does classifies english sentences as erroneous or not, grammar wise. <br /> \n",
    "Using 5% of the colnll14 data: 100 correct + 100 erroneous sentences, because using 8000 sentences or 20% of the data \n",
    "was causing the kernel's death. <br /> \n",
    "Furthermore, two models are used to achieve this classification task: a Random Forrest, and a Multi-Layer Perceptron and that's for the sake of comparaison. <br /> \n",
    "The final conclusion is that the random forrest model outperforms the neural network, which is consistent with the results found by the CS229 collaborators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are two Unix-based commands used to prepare the Colnn14 data to fit the structure requird by 'load_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the files ‘neg’ & ‘pos’ into text files, each containing one sentence. \n",
    "# To be used by the ‘load_files’ function of Sci-Kit Learn\n",
    "\n",
    "split -d -l 1 neg se --additional-suffix=.txt\n",
    "split -d -l 1 pos se --additional-suffix=.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the percentage of the data that you want to use by copying a set number of sentences\n",
    "k=1; find /Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/txt_sentoken_conll14/pos -type f | while read file; do\n",
    "     [[ k++ -le 100 ]] && cp \"$file\" /Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/esmtxt_sentoken_conll14/esmpos \n",
    "done\n",
    "\n",
    "k=1; find /Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/txt_sentoken_conll14/neg -type f | while read file; do\n",
    "     [[ k++ -le 100 ]] && cp \"$file\" /Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/esmtxt_sentoken_conll14/neg \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the path to the folder containing the 'neg' & 'pos' subfolders.\n",
    "# Check the link to get a better understanding of how the magical function 'load_files' functions\n",
    "nucle = load_files(r\"/Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/esmtxt_sentoken_conll14\")\n",
    "X, y = nucle.data, nucle.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'One example is the recent government project of the integrated resort , which will create job opportunities and attract people to spend money when it is completed .\\n', b'Due to the negative consequences of allowing individuals to make the about whether or not to reveal their genetic test results , they should therefore not be given the right to have the final say in the matter .\\n']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(X[1:3])\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r\"[^a-zA-Z/'/]\", ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b' and ending with 'n'\n",
    "    document = re.sub(r\"^b\\'\", '', document)\n",
    "    document = re.sub(r\"^b\", '', document)\n",
    "    document = re.sub(r\"n\\'\", '', document)\n",
    "\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One example is the recent government project of the integrated resort which will create job opportunities and attract people to spend money when it is completed\n"
     ]
    }
   ],
   "source": [
    "le=[]\n",
    "lis=[]\n",
    "li=[]\n",
    "\n",
    "for t in documents:\n",
    "    le = t.split()\n",
    "    lis.append(le)\n",
    "    \n",
    "for e in lis:\n",
    "    while len(e)<25:\n",
    "        e.append('space')\n",
    "\n",
    "texts=[]\n",
    "for e in lis:\n",
    "    str = \" \".join(e)\n",
    "    texts.append(str)\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create POS features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5535, 5)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "l1=[]\n",
    "l2=[]\n",
    "\n",
    "for text in texts:\n",
    "        doc = nlp(text)\n",
    "        l1 = list([token.text, token.pos_, token.tag_, token.dep_, token.shape_] for token in doc)\n",
    "        l2.extend(l1)\n",
    "        arr = np.array(l2)\n",
    "\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode features using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(5535, 1077)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [0, 1, 2, 3, 4])], remainder='passthrough')\n",
    "\n",
    "arr = transformer.fit_transform(arr)\n",
    "print(arr.toarray())\n",
    "print(arr.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the features matrix to be of same rows as sentences' matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "27675.0\n"
     ]
    }
   ],
   "source": [
    "print(arr.toarray().dtype)\n",
    "print(np.sum(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(5, 1192239)\n",
      "27675.0\n"
     ]
    }
   ],
   "source": [
    "arr2d= np.reshape(arr, (5, -1))\n",
    "print(arr2d.toarray())\n",
    "print(arr2d.shape)\n",
    "print(np.sum(arr2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall = np.zeros((5,1192240),dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(wall)\n",
    "print(np.sum(wall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall[:, 0:1192239] += arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1192240)\n",
      "27675.0\n"
     ]
    }
   ],
   "source": [
    "print(wall.shape)\n",
    "print(np.sum(wall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall2d= np.reshape(wall, (200,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 29806)\n",
      "27675.0\n"
     ]
    }
   ],
   "source": [
    "print(wall2d.shape)\n",
    "print(np.sum(wall2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences using 25-dim glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "glove = EmbeddingTransformer('glove') \n",
    "X = glove.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 25)\n",
      "[-1.30604401e-01  2.19229817e-01 -5.71644418e-02  1.40366048e-01\n",
      " -2.00404957e-01 -2.34635159e-01  1.18768394e+00 -4.88223344e-01\n",
      " -1.05908990e-01 -3.86272520e-02 -1.35389075e-01  4.71128464e-01\n",
      " -4.59327841e+00  1.00723945e-01 -2.38876436e-02 -3.94855393e-03\n",
      "  2.80245155e-01 -9.55506191e-02  8.85953382e-02 -4.58416194e-01\n",
      "  8.56998265e-02  4.70229834e-01 -7.97560960e-02 -6.56158403e-02\n",
      " -3.08540136e-01]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features and sentences into one input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate column wise\n",
      "[[-0.00557672  0.12866175 -0.28674632 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0459268   0.33395836 -0.33841449 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.28554589  0.34604859 -0.27926505 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.1303066   0.10723669 -0.47099614 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.42725652  0.42168248 -0.18633699 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05842838  0.136352   -0.11073975 ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"concatenate column wise\")\n",
    "concat = np.concatenate((X,wall2d),axis=1)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final dimensions verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "[0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1\n",
      " 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
      " 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 29831)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training & testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(concat, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Random Forrest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features=20,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_features=20, n_estimators=200, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 18]\n",
      " [ 3 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.18      0.28        22\n",
      "           1       0.45      0.83      0.59        18\n",
      "\n",
      "    accuracy                           0.48        40\n",
      "   macro avg       0.51      0.51      0.43        40\n",
      "weighted avg       0.52      0.47      0.42        40\n",
      "\n",
      "0.475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the FFNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 200, 200, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='adam',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(300,200,200,100), max_iter=500, activation = 'relu', solver='adam', random_state=21, tol=0.000000001)\n",
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 13]\n",
      " [ 9  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.41      0.45        22\n",
      "           1       0.41      0.50      0.45        18\n",
      "\n",
      "    accuracy                           0.45        40\n",
      "   macro avg       0.45      0.45      0.45        40\n",
      "weighted avg       0.46      0.45      0.45        40\n",
      "\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow1)",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
