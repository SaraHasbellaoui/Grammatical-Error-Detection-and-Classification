{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.datasets import load_files\n",
    "import nltk\n",
    "\n",
    "# Pass the path to the folder containing the 'neg' & 'pos' subfolders.\n",
    "# Check the link to get a better understanding of how the magical function 'load_files' functions\n",
    "nucle = load_files(r\"/Users/highsierra/Tech-Skills/Labortory/ML-Fundamentals/esmtxt_sentoken_conll14\")\n",
    "X, y = nucle.data, nucle.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r\"[^a-zA-Z/'/]\", ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b' and ending with 'n'\n",
    "    document = re.sub(r\"^b\\'\", '', document)\n",
    "    document = re.sub(r\"^b\", '', document)\n",
    "    document = re.sub(r\"n\\'\", '', document)\n",
    "\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "post = []\n",
    "full = []\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in documents:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        post.append(token.pos_)\n",
    "    post = []\n",
    "    full.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    while len(full[i]) < 55:\n",
    "        full[i].append('SPACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 55)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 530)\n",
      "[[0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ,\n",
    "14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 ,\n",
    "39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54])], remainder='passthrough')\n",
    "\n",
    "a = transformer.fit_transform(a)\n",
    "print(a.toarray().shape)\n",
    "a = a.toarray()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "glove = EmbeddingTransformer('glove') \n",
    "X = glove.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate column wise\n",
      "[[-0.00557672  0.12866175 -0.28674632 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.0459268   0.33395836 -0.33841449 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.28554589  0.34604859 -0.27926505 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.1303066   0.10723669 -0.47099614 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.42725652  0.42168248 -0.18633699 ...  1.          0.\n",
      "   1.        ]\n",
      " [ 0.05842838  0.136352   -0.11073975 ...  1.          0.\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"concatenate column wise\")\n",
    "concat = np.concatenate((X,a),axis=1)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 555)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(concat, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 555)\n",
    "X_test  = X_test.reshape(-1, 1, 555)\n",
    "y_train = y_train.reshape(-1, 1, )\n",
    "y_test = y_test.reshape(-1, 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(1, 555), dropout=0.2, recurrent_dropout=0.2, return_sequences=True)) \n",
    "model.add(LSTM(100, input_shape=(1, 555), dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/25\n",
      "160/160 [==============================] - 5s 29ms/step - loss: 0.6941 - accuracy: 0.5188\n",
      "Epoch 2/25\n",
      "160/160 [==============================] - 0s 871us/step - loss: 0.6866 - accuracy: 0.5250\n",
      "Epoch 3/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6823 - accuracy: 0.6625\n",
      "Epoch 4/25\n",
      "160/160 [==============================] - 0s 906us/step - loss: 0.6758 - accuracy: 0.7563\n",
      "Epoch 5/25\n",
      "160/160 [==============================] - 0s 871us/step - loss: 0.6647 - accuracy: 0.7250\n",
      "Epoch 6/25\n",
      "160/160 [==============================] - 0s 929us/step - loss: 0.6513 - accuracy: 0.7188\n",
      "Epoch 7/25\n",
      "160/160 [==============================] - 0s 845us/step - loss: 0.6289 - accuracy: 0.7875\n",
      "Epoch 8/25\n",
      "160/160 [==============================] - 0s 930us/step - loss: 0.6053 - accuracy: 0.8125\n",
      "Epoch 9/25\n",
      "160/160 [==============================] - 0s 836us/step - loss: 0.5658 - accuracy: 0.7812\n",
      "Epoch 10/25\n",
      "160/160 [==============================] - 0s 986us/step - loss: 0.5318 - accuracy: 0.8000\n",
      "Epoch 11/25\n",
      "160/160 [==============================] - 0s 978us/step - loss: 0.4888 - accuracy: 0.7937\n",
      "Epoch 12/25\n",
      "160/160 [==============================] - 0s 856us/step - loss: 0.4193 - accuracy: 0.8625\n",
      "Epoch 13/25\n",
      "160/160 [==============================] - 0s 919us/step - loss: 0.3816 - accuracy: 0.8500\n",
      "Epoch 14/25\n",
      "160/160 [==============================] - 0s 907us/step - loss: 0.3345 - accuracy: 0.8813\n",
      "Epoch 15/25\n",
      "160/160 [==============================] - 0s 949us/step - loss: 0.2865 - accuracy: 0.8938\n",
      "Epoch 16/25\n",
      "160/160 [==============================] - 0s 977us/step - loss: 0.3119 - accuracy: 0.8562\n",
      "Epoch 17/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.8938\n",
      "Epoch 18/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8687\n",
      "Epoch 19/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8750\n",
      "Epoch 20/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.8813\n",
      "Epoch 21/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9000\n",
      "Epoch 22/25\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9000\n",
      "Epoch 23/25\n",
      "160/160 [==============================] - 0s 941us/step - loss: 0.1875 - accuracy: 0.9125\n",
      "Epoch 24/25\n",
      "160/160 [==============================] - 0s 975us/step - loss: 0.1450 - accuracy: 0.9438\n",
      "Epoch 25/25\n",
      "160/160 [==============================] - 0s 929us/step - loss: 0.1924 - accuracy: 0.9187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a7c2eed10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow1)",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
