{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Set a fixed seed for the random number generator to address randomness problem and get reproducable results with Keras\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sentences from the data file\n",
    "f=open('/Users/highsierra/Tech-Skills/Unorganised/conll2014-master/release3.2/data/conll14st-preprocessed.m2')\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "Stat = []\n",
    "\n",
    "\n",
    "word_tags=[] # the target output of each word \n",
    "error_tags = [\"ArtOrDet\",\"Nn\",\"Vt\",\"Prep\",\"Vform\",\"Wform\",\"SVA\"] # the seven grammatical errors' tags \n",
    "sen_position=0 \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "for line in f:\n",
    "    parts = line.split()\n",
    "    if(len(parts)>0):\n",
    "        if line[0]=='S':\n",
    "        # Initial Inputs Processing \n",
    "        \n",
    "            # Perform basic cleaning\n",
    "            cleanSen = re.sub(r\"\\n\", '', line[2:])\n",
    "            # Keep track of the sentences' lengths\n",
    "            Stat.append(len(cleanSen.split()))\n",
    "            \n",
    "            # Create a list of sentences\n",
    "            sentences.append(cleanSen)\n",
    "            \n",
    "            # Create a one-dimensional array of input words \n",
    "            words = words + cleanSen.split()\n",
    "            \n",
    "            \n",
    "        # Initial Outputs Processing  \n",
    "            \n",
    "            # By default, consider every word as non-erroneous, by creating an array with the tag \"Correct\" per every word.\n",
    "            tags=np.empty(shape=(len(parts)-1), dtype=object)\n",
    "            tags = np.where(tags==None, \"Correct\", tags)\n",
    "            # Combine the tags associated with each sentence vertically in order to allign them with the input words\n",
    "            word_tags.append(tags)\n",
    "            # Keep track of the sentence's position\n",
    "            sen_position += 1\n",
    "            \n",
    "        elif parts[0]=='A':\n",
    "            if re.findall(\"ArtOrDet\", parts[2]) or re.findall(\"Nn\", parts[2]) or re.findall(\"Vt\", parts[2]) or re.findall(\"Prep\", parts[2]) or re.findall(\"Vform\", parts[2]) or re.findall(\"Wform\", parts[2]) or re.findall(\"SVA\", parts[2]):\n",
    "                # Keep track of the erroneous word's position by extracting it from the sentence annotation \n",
    "                digit = [int(j) for j in re.findall(\"[0-9]+\", parts[2][:2])]            \n",
    "                  \n",
    "                # Extract the erroneous words tag\n",
    "                for tag in error_tags:\n",
    "                    if  re.search(tag, parts[2]):\n",
    "                        err = re.findall(tag, parts[2])\n",
    "                        \n",
    "                # Using its extracted position, place the erroneous word's tag in its sentence\n",
    "                word_tags[sen_position - 1][digit[0]-1] = err[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pos tags for the sentences\n",
    "\n",
    "POSTags = []\n",
    "sentences_POSTags = []\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in sentences:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        POSTags.append(token.pos_)\n",
    "    sentences_POSTags.append(POSTags)\n",
    "    POSTags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sentence of longest length\n",
    "\n",
    "SenStat=[]\n",
    "for ele in sentences_POSTags:\n",
    "    SenStat.append(len(ele))\n",
    "LongestSen = max(SenStat)\n",
    "\n",
    "\n",
    "# Pad the rest of the sentences to be of the same length ie the max length\n",
    "\n",
    "for i in range(len(sentences)): \n",
    "    while len(sentences_POSTags[i]) < LongestSen: \n",
    "        sentences_POSTags[i].append('SPACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the textual features (POS Tags) into numerical features by using the one-hot encoding technique\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 ,\n",
    "16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 ,\n",
    "39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 ,\n",
    "63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 ,\n",
    "87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 ,\n",
    "109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 ,\n",
    "130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 ,148 , 149 ,\n",
    "150 , 151 , 152 , 153 , 154 , 155 , 156 , 157 , 158 , 159 ,160 , 161 , 162 , 163 , 164 , 165 , 166 , 167 , 168 ,169 ,\n",
    "170 , 171 , 172 , 173 , 174 , 175 , 176 , 177 , 178 , 179 , 180 , 181 , 182 , 183 , 184 , 185 , 186 , 187 , 188 , 189 ,\n",
    "190 , 191 , 192 , 193 , 194 , 195 , 196 , 197 , 198 , 199 , 200 ,201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 , 210 ,\n",
    "211 , 212 , 213 , 214 , 215 , 216 , 217 , 218 , 219 , 220 , 221 ,222 , 223 ,224 , 225 , 226 ,227 , 228 , 229 ,230   ])], remainder='passthrough')\n",
    "\n",
    "encoded_sen_POSTags = np.array(sentences_POSTags)\n",
    "encoded_sen_POSTags = transformer.fit_transform(encoded_sen_POSTags)\n",
    "encoded_sen_POSTags = encoded_sen_POSTags.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate number of POS-tagged sentences to be inligned with the input words\n",
    "\n",
    "duplicated_sen_POSTags = []\n",
    "sen=0\n",
    "for nb_of_words in Stat: \n",
    "    for n in range(nb_of_words):\n",
    "        duplicated_sen_POSTags.append(encoded_sen_POSTags[sen])\n",
    "    sen=sen+1\n",
    "duplicated_sen_POSTags = np.array(duplicated_sen_POSTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Encode the input words using GloVe embeddings technique \n",
    "\n",
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "glove = EmbeddingTransformer('glove') \n",
    "encoded_words = glove.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the input words with their original sentence features \n",
    "\n",
    "input_ = np.concatenate((encoded_words,duplicated_sen_POSTags),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tags to a one-dimensional array in order to allign with input words\n",
    "\n",
    "flattened_word_tags = []\n",
    "for sen in word_tags:\n",
    "    for tag in sen:\n",
    "        flattened_word_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target outputs using one-hot encoding technique\n",
    "\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(flattened_word_tags)\n",
    "output_ = lb.transform(flattened_word_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1161567, 2110)\n",
      "(1161567, 8)\n"
     ]
    }
   ],
   "source": [
    "print(input_.shape)\n",
    "print(output_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training 80% and testing 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_, output_, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 2110)\n",
    "X_test  = X_test.reshape(-1, 1, 2110)\n",
    "y_train = y_train.reshape(-1, 1, 8)\n",
    "y_test = y_test.reshape(-1, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "929253/929253 [==============================] - 1843s 2ms/step - loss: 0.1159 - accuracy: 0.9819\n",
      "Epoch 2/3\n",
      "929253/929253 [==============================] - 1708s 2ms/step - loss: 0.1113 - accuracy: 0.9820\n",
      "Epoch 3/3\n",
      "929253/929253 [==============================] - 1686s 2ms/step - loss: 0.1104 - accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a58322050>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.19%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow1)",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
